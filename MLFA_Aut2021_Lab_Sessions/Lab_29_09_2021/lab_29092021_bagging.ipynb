{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGWWA_Qof_tW"
   },
   "source": [
    "# BAGGING\n",
    "\n",
    "Bagging improves the performance of several basic algorithms by making it more robust.\n",
    "\n",
    "We will look at Linear Regression and Decision Tree Regressor model as our base models for Bagging.\n",
    "\n",
    "We will use a housing price dataset (regression problem) with 13 features and 506 data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "OmMjJpbMbXBk"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "BnOIgojnkFEP"
   },
   "outputs": [],
   "source": [
    "# function for a random_generator\n",
    "\n",
    "def random_generator(seed = 0, low = 0, high = None, size = None):\n",
    "    s = seed\n",
    "    a = 11\n",
    "    b = 13\n",
    "\n",
    "    if high is None:\n",
    "        return (\"Error. Upper Limit not found\")\n",
    "    if size is None:\n",
    "        return (\"Error. Size not found\")\n",
    "    if size == 1:\n",
    "        return ((a*s+b)%high)\n",
    "    random_list = []\n",
    "    for i in range(size):\n",
    "        random_list.append((a*s+b)%high)\n",
    "        s = (a*s+b)\n",
    "    return random_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-d4XNpBPfMpk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGRESSION DATASET : \n",
      "      feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0     0.00632      18.0      2.31       0.0     0.538     6.575      65.2   \n",
      "1     0.02731       0.0      7.07       0.0     0.469     6.421      78.9   \n",
      "2     0.02729       0.0      7.07       0.0     0.469     7.185      61.1   \n",
      "3     0.03237       0.0      2.18       0.0     0.458     6.998      45.8   \n",
      "4     0.06905       0.0      2.18       0.0     0.458     7.147      54.2   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "501   0.06263       0.0     11.93       0.0     0.573     6.593      69.1   \n",
      "502   0.04527       0.0     11.93       0.0     0.573     6.120      76.7   \n",
      "503   0.06076       0.0     11.93       0.0     0.573     6.976      91.0   \n",
      "504   0.10959       0.0     11.93       0.0     0.573     6.794      89.3   \n",
      "505   0.04741       0.0     11.93       0.0     0.573     6.030      80.8   \n",
      "\n",
      "     feature8  feature9  feature10  feature11  feature12  feature13  \\\n",
      "0      4.0900       1.0      296.0       15.3     396.90       4.98   \n",
      "1      4.9671       2.0      242.0       17.8     396.90       9.14   \n",
      "2      4.9671       2.0      242.0       17.8     392.83       4.03   \n",
      "3      6.0622       3.0      222.0       18.7     394.63       2.94   \n",
      "4      6.0622       3.0      222.0       18.7     396.90       5.33   \n",
      "..        ...       ...        ...        ...        ...        ...   \n",
      "501    2.4786       1.0      273.0       21.0     391.99       9.67   \n",
      "502    2.2875       1.0      273.0       21.0     396.90       9.08   \n",
      "503    2.1675       1.0      273.0       21.0     396.90       5.64   \n",
      "504    2.3889       1.0      273.0       21.0     393.45       6.48   \n",
      "505    2.5050       1.0      273.0       21.0     396.90       7.88   \n",
      "\n",
      "     price(target)  \n",
      "0             24.0  \n",
      "1             21.6  \n",
      "2             34.7  \n",
      "3             33.4  \n",
      "4             36.2  \n",
      "..             ...  \n",
      "501           22.4  \n",
      "502           20.6  \n",
      "503           23.9  \n",
      "504           22.0  \n",
      "505           11.9  \n",
      "\n",
      "[506 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "# reg_dataset -> Regression Dataset : Boston Dataset with Housing price as the target and 13 Features related to the houses.\n",
    "\n",
    "def load_dataset():\n",
    "    reg_x, reg_y = load_boston(return_X_y = True)\n",
    "    reg_data = np.concatenate((reg_x, np.array(reg_y).reshape(-1, 1)), axis = 1)\n",
    "    cols = [\"feature\"+str(i) for i in range(1, 14)]\n",
    "    cols = cols + [\"price(target)\"]\n",
    "    reg_dataset = pd.DataFrame(data = reg_data, columns = cols)\n",
    "\n",
    "    return reg_dataset\n",
    "\n",
    "reg_dataset = load_dataset()\n",
    "print(\"REGRESSION DATASET : \\n\", reg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "CYYB4xzxzfdz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0     0.00632      18.0      2.31       0.0     0.538     6.575      65.2   \n",
      "1     0.02731       0.0      7.07       0.0     0.469     6.421      78.9   \n",
      "2     0.02729       0.0      7.07       0.0     0.469     7.185      61.1   \n",
      "3     0.03237       0.0      2.18       0.0     0.458     6.998      45.8   \n",
      "4     0.06905       0.0      2.18       0.0     0.458     7.147      54.2   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "501   0.06263       0.0     11.93       0.0     0.573     6.593      69.1   \n",
      "502   0.04527       0.0     11.93       0.0     0.573     6.120      76.7   \n",
      "503   0.06076       0.0     11.93       0.0     0.573     6.976      91.0   \n",
      "504   0.10959       0.0     11.93       0.0     0.573     6.794      89.3   \n",
      "505   0.04741       0.0     11.93       0.0     0.573     6.030      80.8   \n",
      "\n",
      "     feature8  feature9  feature10  feature11  feature12  feature13  \n",
      "0      4.0900       1.0      296.0       15.3     396.90       4.98  \n",
      "1      4.9671       2.0      242.0       17.8     396.90       9.14  \n",
      "2      4.9671       2.0      242.0       17.8     392.83       4.03  \n",
      "3      6.0622       3.0      222.0       18.7     394.63       2.94  \n",
      "4      6.0622       3.0      222.0       18.7     396.90       5.33  \n",
      "..        ...       ...        ...        ...        ...        ...  \n",
      "501    2.4786       1.0      273.0       21.0     391.99       9.67  \n",
      "502    2.2875       1.0      273.0       21.0     396.90       9.08  \n",
      "503    2.1675       1.0      273.0       21.0     396.90       5.64  \n",
      "504    2.3889       1.0      273.0       21.0     393.45       6.48  \n",
      "505    2.5050       1.0      273.0       21.0     396.90       7.88  \n",
      "\n",
      "[506 rows x 13 columns]\n",
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n"
     ]
    }
   ],
   "source": [
    "# Splitting into training and testing\n",
    "\n",
    "cols = [\"feature\"+str(i) for i in range(1, 14)]\n",
    "cols = cols + [\"price(target)\"]\n",
    "\n",
    "X = reg_dataset[cols[:-1]]\n",
    "Y = reg_dataset[cols[-1]]\n",
    "\n",
    "print(X)\n",
    "print(Y.values)\n",
    "\n",
    "TrainX = np.asarray(X)\n",
    "TrainY = np.asarray(Y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(TrainX, TrainY, test_size=0.1, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "954KWjs0rHkC"
   },
   "source": [
    "#### **BAGGING WITH LINEAR REGRESSION MODELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "UkQLqfLFZkiF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 277, 232, 141, 352, 249, 328, 389, 252, 361, 348, 205, 248, 317, 268, 133, 264, 89, 184, 17, 200, 193, 116, 77, 52, 181, 388, 241, 240, 229, 108, 393, 296, 37, 16, 189, 72, 401, 384, 197, 160, 157, 124, 165, 212, 325, 356, 293, 4, 57, 236, 185, 28, 321, 312, 213, 336, 73, 8, 101, 316, 257, 12, 145, 396, 329, 400, 373, 76, 41, 60, 269, 144, 385, 208, 281, 276, 221, 20, 233, 152, 69, 368, 21, 244, 273, 188, 61, 280, 265, 100, 305, 136, 297, 48, 137, 308, 169, 256, 1, 24, 277, 232, 141, 352, 249, 328, 389, 252, 361, 348, 205, 248, 317, 268, 133, 264, 89, 184, 17, 200, 193, 116, 77, 52, 181, 388, 241, 240, 229, 108, 393, 296, 37, 16, 189, 72, 401, 384, 197, 160, 157, 124, 165, 212, 325, 356, 293, 4, 57, 236, 185, 28, 321, 312, 213, 336, 73, 8, 101, 316, 257, 12, 145, 396, 329, 400, 373, 76, 41, 60, 269, 144, 385, 208, 281, 276, 221, 20, 233, 152, 69, 368, 21, 244, 273, 188, 61, 280, 265, 100, 305, 136, 297, 48, 137, 308, 169, 256, 1, 24, 277, 232, 141, 352, 249, 328, 389, 252, 361, 348, 205, 248, 317, 268, 133, 264, 89, 184, 17, 200, 193, 116, 77, 52, 181, 388, 241, 240, 229, 108, 393, 296, 37, 16, 189, 72, 401, 384, 197, 160, 157, 124, 165, 212, 325, 356, 293, 4, 57, 236, 185, 28, 321, 312, 213, 336, 73, 8, 101, 316, 257, 12, 145, 396, 329, 400, 373, 76, 41, 60, 269, 144, 385, 208, 281, 276, 221, 20, 233, 152, 69, 368, 21, 244, 273, 188, 61, 280, 265, 100, 305, 136, 297, 48, 137, 308, 169, 256, 1, 24, 277, 232, 141, 352, 249, 328, 389, 252, 361, 348, 205, 248, 317, 268, 133, 264, 89, 184, 17, 200, 193, 116, 77, 52, 181, 388, 241, 240, 229, 108, 393, 296, 37, 16, 189, 72, 401, 384, 197, 160, 157, 124, 165, 212, 325, 356, 293, 4, 57, 236, 185, 28, 321, 312, 213, 336, 73, 8, 101, 316, 257, 12, 145, 396, 329, 400, 373, 76, 41, 60, 269, 144, 385, 208, 281, 276, 221, 20, 233, 152, 69, 368, 21, 244, 273, 188, 61, 280, 265, 100, 305, 136, 297, 48, 137, 308, 169, 256, 1, 24, 277, 232, 141]\n",
      "404\n",
      "[46, 115, 66, 335, 62, 291, 386, 219, 402, 395, 318, 279, 254, 383, 186, 39, 38, 27, 310, 191, 94, 239, 218, 391, 274, 199, 182, 399, 362, 359, 326, 367, 10, 123, 154, 91, 206, 259, 34, 387, 230, 119, 110, 11, 134, 275, 210, 303, 114, 55, 214, 347, 194, 127, 198, 171, 278, 243, 262, 67, 346, 183, 6, 79, 74, 19, 222, 31, 354, 271, 166, 223, 42, 71, 390, 263, 78, 63, 302, 103, 338, 95, 250, 339, 106, 371, 54, 203, 226, 75, 30, 343, 150, 47, 126, 187, 50, 159, 146, 3, 46, 115, 66, 335, 62, 291, 386, 219, 402, 395, 318, 279, 254, 383, 186, 39, 38, 27, 310, 191, 94, 239, 218, 391, 274, 199, 182, 399, 362, 359, 326, 367, 10, 123, 154, 91, 206, 259, 34, 387, 230, 119, 110, 11, 134, 275, 210, 303, 114, 55, 214, 347, 194, 127, 198, 171, 278, 243, 262, 67, 346, 183, 6, 79, 74, 19, 222, 31, 354, 271, 166, 223, 42, 71, 390, 263, 78, 63, 302, 103, 338, 95, 250, 339, 106, 371, 54, 203, 226, 75, 30, 343, 150, 47, 126, 187, 50, 159, 146, 3, 46, 115, 66, 335, 62, 291, 386, 219, 402, 395, 318, 279, 254, 383, 186, 39, 38, 27, 310, 191, 94, 239, 218, 391, 274, 199, 182, 399, 362, 359, 326, 367, 10, 123, 154, 91, 206, 259, 34, 387, 230, 119, 110, 11, 134, 275, 210, 303, 114, 55, 214, 347, 194, 127, 198, 171, 278, 243, 262, 67, 346, 183, 6, 79, 74, 19, 222, 31, 354, 271, 166, 223, 42, 71, 390, 263, 78, 63, 302, 103, 338, 95, 250, 339, 106, 371, 54, 203, 226, 75, 30, 343, 150, 47, 126, 187, 50, 159, 146, 3, 46, 115, 66, 335, 62, 291, 386, 219, 402, 395, 318, 279, 254, 383, 186, 39, 38, 27, 310, 191, 94, 239, 218, 391, 274, 199, 182, 399, 362, 359, 326, 367, 10, 123, 154, 91, 206, 259, 34, 387, 230, 119, 110, 11, 134, 275, 210, 303, 114, 55, 214, 347, 194, 127, 198, 171, 278, 243, 262, 67, 346, 183, 6, 79, 74, 19, 222, 31, 354, 271, 166, 223, 42, 71, 390, 263, 78, 63, 302, 103, 338, 95, 250, 339, 106, 371, 54, 203, 226, 75, 30, 343, 150, 47, 126, 187, 50, 159, 146, 3, 46, 115, 66, 335]\n",
      "404\n",
      "[68, 357, 304, 125, 176, 333, 40, 49, 148, 25, 288, 353, 260, 45, 104, 349, 216, 369, 32, 365, 392, 285, 320, 301, 92, 217, 380, 153, 80, 85, 140, 341, 128, 209, 292, 397, 340, 117, 88, 173, 300, 81, 96, 261, 56, 225, 64, 313, 224, 53, 192, 105, 360, 337, 84, 129, 220, 9, 112, 33, 376, 109, 0, 13, 156, 113, 44, 93, 228, 97, 272, 177, 344, 161, 168, 245, 284, 309, 180, 377, 120, 121, 132, 253, 372, 65, 324, 345, 172, 289, 364, 381, 164, 201, 204, 237, 196, 149, 36, 5, 68, 357, 304, 125, 176, 333, 40, 49, 148, 25, 288, 353, 260, 45, 104, 349, 216, 369, 32, 365, 392, 285, 320, 301, 92, 217, 380, 153, 80, 85, 140, 341, 128, 209, 292, 397, 340, 117, 88, 173, 300, 81, 96, 261, 56, 225, 64, 313, 224, 53, 192, 105, 360, 337, 84, 129, 220, 9, 112, 33, 376, 109, 0, 13, 156, 113, 44, 93, 228, 97, 272, 177, 344, 161, 168, 245, 284, 309, 180, 377, 120, 121, 132, 253, 372, 65, 324, 345, 172, 289, 364, 381, 164, 201, 204, 237, 196, 149, 36, 5, 68, 357, 304, 125, 176, 333, 40, 49, 148, 25, 288, 353, 260, 45, 104, 349, 216, 369, 32, 365, 392, 285, 320, 301, 92, 217, 380, 153, 80, 85, 140, 341, 128, 209, 292, 397, 340, 117, 88, 173, 300, 81, 96, 261, 56, 225, 64, 313, 224, 53, 192, 105, 360, 337, 84, 129, 220, 9, 112, 33, 376, 109, 0, 13, 156, 113, 44, 93, 228, 97, 272, 177, 344, 161, 168, 245, 284, 309, 180, 377, 120, 121, 132, 253, 372, 65, 324, 345, 172, 289, 364, 381, 164, 201, 204, 237, 196, 149, 36, 5, 68, 357, 304, 125, 176, 333, 40, 49, 148, 25, 288, 353, 260, 45, 104, 349, 216, 369, 32, 365, 392, 285, 320, 301, 92, 217, 380, 153, 80, 85, 140, 341, 128, 209, 292, 397, 340, 117, 88, 173, 300, 81, 96, 261, 56, 225, 64, 313, 224, 53, 192, 105, 360, 337, 84, 129, 220, 9, 112, 33, 376, 109, 0, 13, 156, 113, 44, 93, 228, 97, 272, 177, 344, 161, 168, 245, 284, 309, 180, 377, 120, 121, 132, 253, 372, 65, 324, 345, 172, 289, 364, 381, 164, 201, 204, 237, 196, 149, 36, 5, 68, 357, 304, 125]\n",
      "404\n"
     ]
    }
   ],
   "source": [
    "linreg1 = LinearRegression()\n",
    "row_index = random_generator(1, 0, 404, 404)\n",
    "print(row_index)\n",
    "print(len(row_index))\n",
    "\n",
    "linreg2 = LinearRegression()\n",
    "row_index = random_generator(3, 0, 404, 404)\n",
    "print(row_index)\n",
    "print(len(row_index))\n",
    "\n",
    "linreg3 = LinearRegression()\n",
    "row_index = random_generator(5, 0, 404, 404)\n",
    "print(row_index)\n",
    "print(len(row_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ztDG1UjD1D60"
   },
   "outputs": [],
   "source": [
    "dt1 = DecisionTreeRegressor(max_depth = 3, random_state = 10)\n",
    "row_index = random_generator(8, 0, 404, 404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNR73TaszOypcNLxW6s/ic+",
   "collapsed_sections": [],
   "name": "unit11_Begin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
